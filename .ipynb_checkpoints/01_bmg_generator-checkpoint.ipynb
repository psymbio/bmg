{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 08:17:06.159727: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-18 08:17:06.411730: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-18 08:17:06.954329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 08:17:06.954469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-18 08:17:06.954479: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from PyAstronomy import pyasl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, \n",
    "    MinMaxScaler, \n",
    "    MaxAbsScaler, \n",
    "    # RobustScalar,\n",
    "    Normalizer,\n",
    "    QuantileTransformer,\n",
    "    PowerTransformer,\n",
    "    OneHotEncoder, \n",
    "    OrdinalEncoder,\n",
    "    LabelEncoder\n",
    ")\n",
    "\n",
    "from sklearn.utils import all_estimators\n",
    "\n",
    "from sklearn.base import (\n",
    "    RegressorMixin, \n",
    "    ClassifierMixin,\n",
    "    TransformerMixin\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    r2_score,\n",
    "    mean_squared_error,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import warnings\n",
    "import xgboost\n",
    "import catboost\n",
    "import lightgbm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.2f\" % x)\n",
    "\n",
    "removed_classifiers = [\n",
    "    \"ClassifierChain\",\n",
    "    \"ComplementNB\",\n",
    "    # \"GradientBoostingClassifier\",\n",
    "    \"GaussianProcessClassifier\",\n",
    "    \"HistGradientBoostingClassifier\",\n",
    "    # \"MLPClassifier\",\n",
    "    \"LogisticRegressionCV\", \n",
    "    \"MultiOutputClassifier\", \n",
    "    \"MultinomialNB\", \n",
    "    \"OneVsOneClassifier\",\n",
    "    \"OneVsRestClassifier\",\n",
    "    \"OutputCodeClassifier\",\n",
    "    \"RadiusNeighborsClassifier\",\n",
    "    \"VotingClassifier\",\n",
    "    \"CategoricalNB\",\n",
    "    \"StackingClassifier\",\n",
    "    \"NuSVC\",\n",
    "]\n",
    "\n",
    "removed_regressors = [\n",
    "    \"TheilSenRegressor\",\n",
    "    \"ARDRegression\", \n",
    "    \"CCA\", \n",
    "    \"IsotonicRegression\", \n",
    "    \"StackingRegressor\",\n",
    "    \"MultiOutputRegressor\", \n",
    "    \"MultiTaskElasticNet\", \n",
    "    \"MultiTaskElasticNetCV\", \n",
    "    \"MultiTaskLasso\", \n",
    "    \"MultiTaskLassoCV\", \n",
    "    \"PLSCanonical\", \n",
    "    \"PLSRegression\", \n",
    "    \"RadiusNeighborsRegressor\", \n",
    "    \"RegressorChain\", \n",
    "    \"VotingRegressor\", \n",
    "]\n",
    "\n",
    "CLASSIFIERS = [\n",
    "    est\n",
    "    for est in all_estimators()\n",
    "    if (issubclass(est[1], ClassifierMixin) and (est[0] not in removed_classifiers))\n",
    "]\n",
    "\n",
    "\n",
    "REGRESSORS = [\n",
    "    est\n",
    "    for est in all_estimators()\n",
    "    if (issubclass(est[1], RegressorMixin) and (est[0] not in removed_regressors))\n",
    "]\n",
    "\n",
    "REGRESSORS.append((\"XGBRegressor\", xgboost.XGBRegressor))\n",
    "REGRESSORS.append((\"LGBMRegressor\", lightgbm.LGBMRegressor))\n",
    "REGRESSORS.append(('CatBoostRegressor', catboost.CatBoostRegressor))\n",
    "\n",
    "CLASSIFIERS.append((\"XGBClassifier\", xgboost.XGBClassifier))\n",
    "CLASSIFIERS.append((\"LGBMClassifier\", lightgbm.LGBMClassifier))\n",
    "CLASSIFIERS.append(('CatBoostClassifier', catboost.CatBoostClassifier))\n",
    "\n",
    "TRANSFOMER_METHODS = [\n",
    "    (\"StandardScaler\", StandardScaler), \n",
    "    (\"MinMaxScaler\", MinMaxScaler), \n",
    "    (\"MaxAbsScaler\", MaxAbsScaler), \n",
    "    # (\"RobustScalar\", RobustScalar),\n",
    "    (\"Normalizer\", Normalizer),\n",
    "    (\"QuantileTransformer\", QuantileTransformer),\n",
    "    (\"PowerTransformer\", PowerTransformer),\n",
    "]\n",
    "\n",
    "def adjusted_rsquared(r2, n, p):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - p - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://discuss.huggingface.co/t/what-does-increasing-number-of-heads-do-in-the-multi-head-attention/1847/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmg_alloy</th>\n",
       "      <th>delta_h_mix</th>\n",
       "      <th>delta_s_mix</th>\n",
       "      <th>delta_d</th>\n",
       "      <th>delta_e</th>\n",
       "      <th>actual_d_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Al92Nd6Ni2</td>\n",
       "      <td>10.61</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zr26Ti10Cu8Ni8Be20Y4Mg24</td>\n",
       "      <td>13.89</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gd55Ni22Mn3Al20</td>\n",
       "      <td>11.82</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ag53.8Mg15.4Ca30.8</td>\n",
       "      <td>10.05</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ni60Nb20Zr20</td>\n",
       "      <td>19.88</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Mg65Cu7.5Ni7.5Ag5Zn5Gd7.5Y2.5</td>\n",
       "      <td>9.89</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>13.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Mg65Cu7.5Ni7.5Ag5Zn5Gd2.5Y7.5</td>\n",
       "      <td>9.96</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>Mg80Ni10Nd10</td>\n",
       "      <td>9.39</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>Cu45.5Mg31.8Ca22.7</td>\n",
       "      <td>10.66</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.34</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>Mg59.5Cu22.9Ag6.6Gd11</td>\n",
       "      <td>10.02</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>27.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1059 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bmg_alloy  delta_h_mix  delta_s_mix  delta_d  \\\n",
       "0                        Al92Nd6Ni2        10.61         0.32     0.20   \n",
       "1          Zr26Ti10Cu8Ni8Be20Y4Mg24        13.89         1.78     0.17   \n",
       "2                   Gd55Ni22Mn3Al20        11.82         1.09     0.26   \n",
       "3                Ag53.8Mg15.4Ca30.8        10.05         0.98     0.25   \n",
       "4                      Ni60Nb20Zr20        19.88         0.95     0.17   \n",
       "...                             ...          ...          ...      ...   \n",
       "1054  Mg65Cu7.5Ni7.5Ag5Zn5Gd7.5Y2.5         9.89         1.25     0.18   \n",
       "1055  Mg65Cu7.5Ni7.5Ag5Zn5Gd2.5Y7.5         9.96         1.25     0.17   \n",
       "1056                   Mg80Ni10Nd10         9.39         0.64     0.25   \n",
       "1057             Cu45.5Mg31.8Ca22.7        10.66         1.06     0.34   \n",
       "1058          Mg59.5Cu22.9Ag6.6Gd11        10.02         1.07     0.20   \n",
       "\n",
       "      delta_e  actual_d_max  \n",
       "0       -0.17          0.00  \n",
       "1       -0.12          5.00  \n",
       "2       -0.18          2.00  \n",
       "3       -0.28          0.80  \n",
       "4       -0.13          0.50  \n",
       "...       ...           ...  \n",
       "1054    -0.16         13.00  \n",
       "1055    -0.16          9.50  \n",
       "1056    -0.09          0.60  \n",
       "1057    -0.25          1.25  \n",
       "1058    -0.19         27.00  \n",
       "\n",
       "[1059 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bmg_alloy</th>\n",
       "      <th>delta_h_mix</th>\n",
       "      <th>delta_s_mix</th>\n",
       "      <th>delta_d</th>\n",
       "      <th>delta_e</th>\n",
       "      <th>actual_d_max</th>\n",
       "      <th>predicted_d_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Co14Ni69P17</td>\n",
       "      <td>14.24</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C14Fe66W20</td>\n",
       "      <td>30.81</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cu15La30Mg55</td>\n",
       "      <td>8.64</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Al87.5Ca2.5Ni10</td>\n",
       "      <td>11.30</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B36C8Co56</td>\n",
       "      <td>35.47</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Gd55Ni25Al20</td>\n",
       "      <td>11.94</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Mg65Ni20La15</td>\n",
       "      <td>10.04</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>B8Fe89Ni3</td>\n",
       "      <td>16.80</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>La62Al14Cu18Ag6</td>\n",
       "      <td>8.44</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Ag53.8Mg15.4Ca23.1Cu7.7</td>\n",
       "      <td>10.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   bmg_alloy  delta_h_mix  delta_s_mix  delta_d  delta_e  \\\n",
       "0                Co14Ni69P17        14.24         0.83     0.12     0.04   \n",
       "1                 C14Fe66W20        30.81         0.87     0.14    -0.13   \n",
       "2               Cu15La30Mg55         8.64         0.97     0.31    -0.13   \n",
       "3            Al87.5Ca2.5Ni10        11.30         0.44     0.21    -0.24   \n",
       "4                  B36C8Co56        35.47         0.89     0.11    -0.14   \n",
       "..                       ...          ...          ...      ...      ...   \n",
       "257             Gd55Ni25Al20        11.94         1.00     0.27    -0.19   \n",
       "258             Mg65Ni20La15        10.04         0.89     0.29    -0.14   \n",
       "259                B8Fe89Ni3        16.80         0.41     0.16    -0.02   \n",
       "260          La62Al14Cu18Ag6         8.44         1.05     0.36    -0.23   \n",
       "261  Ag53.8Mg15.4Ca23.1Cu7.7        10.40         1.16     0.19    -0.26   \n",
       "\n",
       "     actual_d_max  predicted_d_max  \n",
       "0            0.00             0.02  \n",
       "1            0.00             0.00  \n",
       "2            0.00             1.77  \n",
       "3            0.00            -0.01  \n",
       "4            0.00            -0.03  \n",
       "..            ...              ...  \n",
       "257          2.00             1.36  \n",
       "258          0.50             0.44  \n",
       "259          0.00             0.02  \n",
       "260          5.00             5.77  \n",
       "261          0.50             1.22  \n",
       "\n",
       "[262 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train = pd.read_excel(\"dataset.xlsx\", sheet_name=\"train\")\n",
    "df_test = pd.read_excel(\"dataset.xlsx\", sheet_name=\"test\")\n",
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Encode the bmg_alloys using an RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "\n",
    "n_elements = 118\n",
    "alloy_max_len = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_factor = max(max(df_train[\"actual_d_max\"]), max(df_test[\"actual_d_max\"]))\n",
    "\n",
    "print(max_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic utility functions to calculate features\n",
    "\n",
    "def get_elements_and_compositions(x, verbose = -1):\n",
    "    # seperating atoms from composition\n",
    "    s = re.sub(r'[^\\w\\s]','',x)\n",
    "    s = re.sub('\\d',' ',s)\n",
    "    elements = np.array([i for i in s.split(' ') if i != \"\"])\n",
    "    if verbose > 0:\n",
    "        print('\\nElements in BMG are : ', elements)\n",
    "\n",
    "    compositions = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)\n",
    "    compositions = [float(i) for i in compositions]\n",
    "    if verbose > 0:\n",
    "        print('Compositions: ', compositions)\n",
    "    \n",
    "    return elements, compositions\n",
    "\n",
    "def diff(alloy):\n",
    "    # making ranges for each atom\n",
    "    ranges = {}\n",
    "    for i in alloy:\n",
    "        ranges[i] = 0.88 * parameters[i][\"ar\"]\n",
    "    # compiling scoring matrix\n",
    "    score = {}\n",
    "    for i in alloy:\n",
    "        current_score = {}\n",
    "        for j in alloy:\n",
    "            if parameters[i][\"ar\"] < ranges[j]:\n",
    "                current_score[j] = -1\n",
    "            elif parameters[i][\"ar\"] > parameters[j][\"ar\"]:\n",
    "                current_score[j] = 1\n",
    "            else:\n",
    "                current_score[j] = 0\n",
    "        score[i] = current_score\n",
    "\n",
    "    big = []\n",
    "    small = []\n",
    "    # separating into big and small based on scoring matrix\n",
    "    for i in score:\n",
    "        total_sum = 0\n",
    "        for j in score[i]:\n",
    "            total_sum = total_sum + score[i][j]\n",
    "        if total_sum > 0:\n",
    "            big.append(i)\n",
    "        else:\n",
    "            small.append(i)\n",
    "\n",
    "    if len(big) == 0 or len(small) == 0:\n",
    "        print(score)\n",
    "    return big, small\n",
    "    \n",
    "# finds the paramater deltaE\n",
    "def electro(elements_and_compositions, alloy):\n",
    "    summation_of_product_of_composition_and_electronegativity = 0 # summation of product of composition and electro negativity\n",
    "    summation_of_composition = 0 # summation of compositions\n",
    "    for i in elements_and_compositions:\n",
    "        if i in alloy:\n",
    "            summation_of_composition = summation_of_composition + elements_and_compositions[i]\n",
    "            \n",
    "    for i in elements_and_compositions:\n",
    "        if i in alloy:\n",
    "            summation_of_product_of_composition_and_electronegativity = summation_of_product_of_composition_and_electronegativity + elements_and_compositions[i] * parameters[i]['en']\n",
    "    return summation_of_product_of_composition_and_electronegativity / summation_of_composition\n",
    "\n",
    "# finds the paramater deltaD\n",
    "def comps(elements_and_compositions, alloy):\n",
    "    summation_of_product_of_composition_and_atomicradii = 0 # summation of product of composition and atomic radii\n",
    "    summation_of_compositions = 0 # summation of compositions\n",
    "    for i in elements_and_compositions:\n",
    "        if i in alloy:\n",
    "            summation_of_compositions = summation_of_compositions + elements_and_compositions[i]\n",
    "    for i in elements_and_compositions:\n",
    "        if i in alloy:\n",
    "            summation_of_product_of_composition_and_atomicradii = summation_of_product_of_composition_and_atomicradii + elements_and_compositions[i] * parameters[i]['ar']\n",
    "    return summation_of_product_of_composition_and_atomicradii / summation_of_compositions\n",
    "\n",
    "def prepare_params(alloy):\n",
    "    # seperating atoms from composition\n",
    "    s = re.sub(r'[^\\w\\s]','', alloy)\n",
    "    s = re.sub('\\d', ' ', s)\n",
    "    elements = np.array([i for i in s.split(' ') if i in parameters]) # elements list\n",
    "    # print('\\nElements in BMG are : ', elements)\n",
    "\n",
    "    compositions = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", alloy)\n",
    "    compositions = [float(i) for i in compositions]\n",
    "    # print('Compositions: ', compositions)\n",
    "\n",
    "    elements_and_compositions = dict(zip(elements, compositions))\n",
    "    s_mix = 0\n",
    "    h_mix = 0\n",
    "\n",
    "    for i in elements_and_compositions:\n",
    "        s_mix = s_mix + (elements_and_compositions[i] / 100) * (math.log((elements_and_compositions[i] / 100)))\n",
    "        h_mix = h_mix + (elements_and_compositions[i] / 100) * parameters[i]['enthalphy']\n",
    "    s_mix = -1*s_mix\n",
    "\n",
    "    big, small = diff(elements)\n",
    "    # print(\"big atoms : \", big)\n",
    "    # print(\"small atoms : \", small)\n",
    "    delta_d = (comps(elements_and_compositions,big) - comps(elements_and_compositions,small)) / (comps(elements_and_compositions,big))\n",
    "    delta_e = (electro(elements_and_compositions,big) - electro(elements_and_compositions,small)) / (electro(elements_and_compositions,big) + electro(elements_and_compositions,small))\n",
    "    return h_mix, s_mix, delta_d, delta_e, elements[np.argmax(compositions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions for encoding\n",
    "\n",
    "# find element index from all_elements, e.g. \"He\" = 2\n",
    "def element_to_index(element):\n",
    "    try:\n",
    "        atomic_number = pyasl.AtomicNo()\n",
    "        return atomic_number.getAtomicNo(element)\n",
    "    except:\n",
    "        return \"END\"\n",
    "    \n",
    "def index_to_element(index):\n",
    "    try:\n",
    "        atomic_number = pyasl.AtomicNo()\n",
    "        return atomic_number.getElSymbol(index)\n",
    "    except:\n",
    "        return \"END\"\n",
    "    \n",
    "def alloy_to_1d_tensor(alloy_str, alloy_max_len = alloy_max_len):\n",
    "    tensor = torch.zeros(alloy_max_len)\n",
    "    elements, compositions = get_elements_and_compositions(alloy_str)\n",
    "    i = 0\n",
    "    for idx in range(0, len(elements) + len(compositions), 2):\n",
    "        tensor[idx] = element_to_index(elements[i])\n",
    "        tensor[idx + 1] = compositions[i]\n",
    "        i += 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1059, 20]) torch.float32 torch.Size([1059]) 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# creating the input and output data for the model\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(df_train.shape[0]):\n",
    "    X_train.append(alloy_to_1d_tensor(df_train.loc[i, \"bmg_alloy\"]))\n",
    "    y_train.append(df_train.loc[i, \"actual_d_max\"] / max_factor)\n",
    "\n",
    "X_train = torch.stack(X_train)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "print(X_train.shape, X_train.dtype, y_train.shape, min(y_train).item(), max(y_train).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([262, 20]) 0.0 0.6666666865348816\n"
     ]
    }
   ],
   "source": [
    "for i in range(df_test.shape[0]):\n",
    "    X_test.append(alloy_to_1d_tensor(df_test.loc[i, \"bmg_alloy\"]))\n",
    "    y_test.append(df_test.loc[i, \"actual_d_max\"] / max_factor)\n",
    "    \n",
    "# max_factor we selected is fine\n",
    "X_test = torch.stack(X_test)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "print(X_test.shape, min(y_test).item(), max(y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# class SequenceToSigmoid(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = True)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.lstm(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "class SequenceToSigmoid(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.rnn = nn.RNN(self.input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.output_size)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.rnn(x)\n",
    "        output = self.linear(hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "# class SequenceToSigmoidMultiheadedAttention(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, bidirectional=True)\n",
    "#         self.multihead_attention = nn.MultiheadAttention(hidden_size * 2, num_heads)\n",
    "#         self.fc = nn.Linear(hidden_size * 2, 1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x, _ = self.lstm(x)\n",
    "\n",
    "#         # Permute x to match the expected shape for multihead attention\n",
    "#         print(x.shape)\n",
    "#         x = x.permute(1, 0, 2)\n",
    "\n",
    "#         # Apply multihead attention\n",
    "#         x, _ = self.multihead_attention(x, x, x)\n",
    "\n",
    "#         # Permute x back to the original shape\n",
    "#         x = x.permute(1, 0, 2)\n",
    "\n",
    "#         x = self.fc(x)\n",
    "#         x = self.sigmoid(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Calculate the loss\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# print(\"output\", outputs.squeeze(), \"\\nlabel\", labels)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mSequenceToSigmoidMultiheadedAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     84\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch_size, hidden_size * 2) -> (batch_size, seq_len, hidden_size * 2)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Apply multihead attention\u001b[39;00m\n\u001b[1;32m     88\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultihead_attention(x, x, x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "# Bidirectional LSTM\n",
    "# Define hyperparameters\n",
    "input_size = 20\n",
    "hidden_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "num_layers = 3\n",
    "num_heads = 4\n",
    "\n",
    "# Create an instance of the model\n",
    "\n",
    "model = SequenceToSigmoid(input_size, hidden_size, num_layers)\n",
    "# model = SequenceToSigmoidMultiheadedAttention(input_size, hidden_size, num_layers, num_heads)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "\n",
    "# Create a PyTorch DataLoader for batching the data\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set the device to use (CPU or GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print_every = 100\n",
    "# TODO: print the average loss\n",
    "# EDIT: we are printing epoch loss of batch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over the batches of data\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        # print(\"output\", outputs.squeeze(), \"\\nlabel\", labels)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    if epoch % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3.6298945e-07\n",
      "0.0 1.8619472e-05\n",
      "0.0 6.7568158e-06\n",
      "0.0 3.3708442e-05\n",
      "0.0 1.4590311e-06\n",
      "0.23333333 0.2679993\n",
      "0.0 9.892632e-07\n",
      "0.0 0.026995046\n",
      "0.0 2.5583697e-06\n",
      "0.1 0.30679902\n",
      "0.0 1.1703495e-06\n",
      "0.0 1.4798121e-05\n",
      "0.0 8.1757156e-05\n",
      "0.0 7.669599e-07\n",
      "0.0 3.6673939e-07\n",
      "0.0 1.359098e-06\n",
      "0.0 2.0678958e-06\n",
      "0.0 3.745091e-06\n",
      "0.36666667 0.30365738\n",
      "0.0 1.6171422e-06\n",
      "0.0 7.040739e-07\n",
      "0.13333334 0.16141804\n",
      "0.0 2.122186e-06\n",
      "0.083333336 0.07492791\n",
      "0.0 6.058353e-07\n",
      "0.0 6.147599e-07\n",
      "0.16666667 0.14631484\n",
      "0.33333334 0.4263134\n",
      "0.0 7.5890483e-07\n",
      "0.06666667 0.06416459\n",
      "0.13333334 0.072222754\n",
      "0.016666668 0.12380588\n",
      "0.0 5.052038e-07\n",
      "0.06666667 0.042509995\n",
      "0.26666668 0.077733874\n",
      "0.0 1.9609224e-06\n",
      "0.26666668 0.41705057\n",
      "0.13333334 0.3237047\n",
      "0.1 0.08714993\n",
      "0.1 0.05120107\n",
      "0.05 0.06958216\n",
      "0.0 2.9912595e-05\n",
      "0.13333334 0.17029697\n",
      "0.0 1.1063446e-06\n",
      "0.0 2.0188463e-05\n",
      "0.0 0.0003852168\n",
      "0.0 9.157918e-06\n",
      "0.0 3.751411e-06\n",
      "0.06666667 0.05227725\n",
      "0.0 1.1208248e-06\n",
      "0.13333334 0.34722996\n",
      "0.0 2.4799742e-06\n",
      "0.26666668 0.17470662\n",
      "0.05 0.013329379\n",
      "0.1 0.063100316\n",
      "0.06666667 0.06159821\n",
      "0.06666667 0.07648021\n",
      "0.06666667 0.050471477\n",
      "0.0 4.2164957e-06\n",
      "0.0 1.2802679e-05\n",
      "0.26666668 0.30498886\n",
      "0.0 3.933762e-06\n",
      "0.16666667 0.23632815\n",
      "0.05 0.07262195\n",
      "0.2 0.23735\n",
      "0.2 0.28065875\n",
      "0.13333334 0.33115143\n",
      "0.33333334 0.31941912\n",
      "0.0 2.6972334e-06\n",
      "0.0 3.082052e-06\n",
      "0.13333334 0.17678015\n",
      "0.1 0.12618837\n",
      "0.0 3.8792328e-07\n",
      "0.0 2.0533853e-06\n",
      "0.26666668 0.26613867\n",
      "0.0 3.9988383e-05\n",
      "0.0 2.3894552e-06\n",
      "0.1 0.14407787\n",
      "0.06666667 0.073944695\n",
      "0.33333334 0.17730336\n",
      "0.0 3.0406425e-06\n",
      "0.0 2.1671444e-06\n",
      "0.0 7.2177804e-06\n",
      "0.0 7.8030996e-07\n",
      "0.0 7.054631e-06\n",
      "0.11666667 0.088198304\n",
      "0.0 2.572835e-06\n",
      "0.6666667 0.81919503\n",
      "0.2 0.34101942\n",
      "0.06666667 0.0469017\n",
      "0.016666668 0.10306206\n",
      "0.1 0.15934756\n",
      "0.0 2.9513162e-06\n",
      "0.0 4.088787e-06\n",
      "0.16666667 0.1445822\n",
      "0.0 1.4021481e-06\n",
      "0.16666667 0.06325842\n",
      "0.05 0.045913365\n",
      "0.4 0.112653784\n",
      "0.1 0.13718435\n",
      "0.33333334 0.16162796\n",
      "0.0 2.05391e-06\n",
      "0.0 1.9248278e-06\n",
      "0.0 3.8008784e-05\n",
      "0.0 3.121609e-07\n",
      "0.0 9.21627e-07\n",
      "0.0 7.9620116e-07\n",
      "0.0 1.6140392e-07\n",
      "0.0 2.3880264e-07\n",
      "0.033333335 0.019905724\n",
      "0.0 2.7578963e-07\n",
      "0.16666667 0.122079104\n",
      "0.0 3.1134614e-06\n",
      "0.2 0.10350358\n",
      "0.33333334 0.34823516\n",
      "0.0 1.5930352e-06\n",
      "0.0 1.07873e-06\n",
      "0.26666668 0.09163397\n",
      "0.0 4.6035675e-06\n",
      "0.033333335 0.061185803\n",
      "0.06666667 0.038991425\n",
      "0.033333335 0.01660698\n",
      "0.26666668 0.13730304\n",
      "0.0 1.6666104e-06\n",
      "0.0 6.9148323e-06\n",
      "0.0 5.558047e-07\n",
      "0.11666667 0.08265606\n",
      "0.0 3.5727871e-06\n",
      "0.06666667 0.04883088\n",
      "0.0 1.1018675e-06\n",
      "0.0 1.5938886e-05\n",
      "0.21666667 0.22463317\n",
      "0.05 0.0065239207\n",
      "0.083333336 0.052695774\n",
      "0.5 0.6893036\n",
      "0.06666667 0.050649032\n",
      "0.0 7.5327075e-06\n",
      "0.0 1.3452652e-06\n",
      "0.0 4.372258e-07\n",
      "0.0 5.070683e-06\n",
      "0.0 1.2046022e-05\n",
      "0.13333334 0.14245449\n",
      "0.0 1.1553024e-06\n",
      "0.06666667 0.060994487\n",
      "0.1 0.12809452\n",
      "0.16666667 0.20136298\n",
      "0.0 1.13519825e-07\n",
      "0.0 6.3690817e-07\n",
      "0.0 1.1050825e-05\n",
      "0.0 0.00011135347\n",
      "0.0 6.4699752e-06\n",
      "0.0 4.5673855e-06\n",
      "0.016666668 0.014658585\n",
      "0.0 0.00019783001\n",
      "0.13333334 0.24298756\n",
      "0.016666668 0.10284784\n",
      "0.06666667 0.06715603\n",
      "0.0 1.046258e-06\n",
      "0.0 1.6455563e-06\n",
      "0.0 1.3275057e-06\n",
      "0.16666667 0.14628103\n",
      "0.0 1.5684003e-06\n",
      "0.0 0.0091775935\n",
      "0.0 0.0025803493\n",
      "0.16666667 0.16475324\n",
      "0.0 5.4195027e-07\n",
      "0.0 1.2346474e-06\n",
      "0.1 0.07040176\n",
      "0.0 7.4429516e-07\n",
      "0.16666667 0.29607642\n",
      "0.06666667 4.4992344e-06\n",
      "0.1 0.10710247\n",
      "0.06666667 0.05011545\n",
      "0.0 1.9514572e-07\n",
      "0.33333334 0.55939\n",
      "0.26666668 0.07974328\n",
      "0.33333334 0.35594872\n",
      "0.16666667 0.23442943\n",
      "0.0 5.5892347e-06\n",
      "0.0 5.730302e-07\n",
      "0.06666667 0.07295353\n",
      "0.026666667 0.034271386\n",
      "0.0 1.6921189e-05\n",
      "0.33333334 0.26031265\n",
      "0.0 4.0392406e-06\n",
      "0.06666667 0.117366195\n",
      "0.06666667 0.049431026\n",
      "0.0 6.5934887e-06\n",
      "0.0 6.37652e-05\n",
      "0.0 3.1255754e-06\n",
      "0.0 3.4203305e-07\n",
      "0.0 5.913684e-07\n",
      "0.0 1.2951227e-06\n",
      "0.13333334 0.1251947\n",
      "0.0 2.0088662e-06\n",
      "0.0 1.1747004e-06\n",
      "0.16666667 0.14255363\n",
      "0.0 1.4028613e-05\n",
      "0.1 0.109092176\n",
      "0.13333334 0.20074263\n",
      "0.033333335 0.051085915\n",
      "0.0 6.55201e-05\n",
      "0.083333336 0.059870597\n",
      "0.5 0.7069852\n",
      "0.0 3.9591478e-07\n",
      "0.3 0.26419568\n",
      "0.06666667 0.06639434\n",
      "0.0 6.494635e-06\n",
      "0.4 0.41132057\n",
      "0.0 3.0991868e-07\n",
      "0.0 1.0268693e-05\n",
      "0.1 0.1709322\n",
      "0.53333336 0.2614665\n",
      "0.16666667 0.077123806\n",
      "0.33333334 0.52167803\n",
      "0.26666668 0.13463269\n",
      "0.33333334 0.11497965\n",
      "0.016666668 0.026618183\n",
      "0.0 1.5972978e-06\n",
      "0.0 2.8492917e-07\n",
      "0.1 0.1222379\n",
      "0.13333334 0.10652843\n",
      "0.16666667 0.053124223\n",
      "0.0 3.6389625e-05\n",
      "0.0 9.763403e-07\n",
      "0.33333334 0.25923228\n",
      "0.0 4.352958e-07\n",
      "0.0 4.0006806e-05\n",
      "0.11666667 0.21590318\n",
      "0.0 3.8956596e-06\n",
      "0.26666668 0.2684743\n",
      "0.0 3.963057e-06\n",
      "0.0 4.6782592e-05\n",
      "0.0 9.240226e-07\n",
      "0.06666667 0.07288867\n",
      "0.033333335 0.16789986\n",
      "0.0 2.7574017e-06\n",
      "0.16666667 0.21551357\n",
      "0.0 9.1122513e-07\n",
      "0.06666667 0.059574865\n",
      "0.0 2.348601e-07\n",
      "0.1 0.07676776\n",
      "0.0 3.7020454e-06\n",
      "0.0 9.4028144e-07\n",
      "0.0 1.2720474e-06\n",
      "0.0 2.5894258e-06\n",
      "0.3 0.22144052\n",
      "0.0 4.555649e-06\n",
      "0.16666667 0.03800669\n",
      "0.0 3.4004627e-06\n",
      "0.13333334 0.17072621\n",
      "0.0 1.1426466e-06\n",
      "0.0 2.0534437e-06\n",
      "0.13333334 0.09504508\n",
      "0.033333335 0.23535636\n",
      "0.26666668 0.2580011\n",
      "0.033333335 0.3455326\n",
      "0.06666667 0.11806408\n",
      "0.016666668 0.08808653\n",
      "0.0 3.8991125e-06\n",
      "0.16666667 0.17078899\n",
      "0.016666668 0.025938315\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have a separate validation dataset X_val and y_val\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Convert the validation data to PyTorch tensors\n",
    "X_val_tensor = torch.Tensor(X_test)\n",
    "y_val_tensor = torch.Tensor(y_test)\n",
    "\n",
    "# Create a PyTorch DataLoader for batching the validation data\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Append the predicted and true labels to the lists\n",
    "        predicted_labels.extend(outputs.squeeze().cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate the R2 score\n",
    "for i in range(len(true_labels)):\n",
    "    print(true_labels[i], predicted_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.6616\n"
     ]
    }
   ],
   "source": [
    "r2 = r2_score(np.array(true_labels)*max_factor, np.array(predicted_labels)*max_factor)\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(x=np.array(true_labels)*max_factor, y=np.array(predicted_labels)*max_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
